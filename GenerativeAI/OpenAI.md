1. AI can be categorized into two sections - Generative AI and Non-Generative AI
    1. Generative AI - New Content ( Text, images, videos and audio ) will be generated using ML models. 
    2. Non-Generative AI - Classification and Regression ML models
2. Language Model is an AI model that can oredict the next word ( or set of words) for a given sequence of words. When a huge amount of data is fed to a language model, like loads of news, books etc, it becomes a Large Language Model.
3. GPT = Generative Pre-trained Transformers
4. Text to text models = BERT (Google), GPT (OpenAI)
5. Text to Image models = DALL-E and Stable Difusion
6. Text to video - OpenAI Sora
7. Critical breakthrough in GenAI came with the research paper = "Attention is all you need" which introduced a special neural network model named "Transformers".
8. Statistical ML ==> Neural Networks ==> RNN ==> Transformers
9. Stochastic means a system which is characterised by randomness or probability. 
10. Examples of LLM - 
    1. PaLM2 by Google
    2. LLaMA by Facebook (Meta)
    3. ChatGPT by OpenAI
11. While training ChatGPT, OpenAI used Reinforcement Learning with Human Feedback (RLHF). OpenAI used huge intervention of humans to make ChatGPT less toxic. 
12. Embeddings is a numeric representation of text in form of a vector such that you can capture the meaning of the text. Vector Database allows to store these embeddings and perform efficient search on these embeddings. Semantic search is understanding the intent of the user query and using the context of the user query to perform the search. 
12. BERT and GPT are based on transformer architecture. ElMO based on LSTM.
13. Retrieval Augmented Generation (RAG) - 